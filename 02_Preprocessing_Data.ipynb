{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/becausejustyn/xai_ppa/blob/main/notebooks/preprocessing_faces.ipynb)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create a new directory for `data/dark_undersampled/` and `data/light_undersampled/` that will be cropped and downsized to `(256, 256)`. The file size is much smaller once resized so I am not too concerned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_COLAB = 'google.colab' in str(get_ipython())\n",
    "\n",
    "if ON_COLAB:\n",
    "    !pip install -q facenet_pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if running on colab, mount google drive\n",
    "\n",
    "if ON_COLAB:\n",
    "    from google.colab import drive\n",
    "\n",
    "    # checking if drive is mounted\n",
    "    try:\n",
    "        with open('/content/drive/My Drive/test.txt') as f:\n",
    "            print('Google Drive is already mounted.')\n",
    "    except FileNotFoundError:\n",
    "        drive.mount('/content/drive')\n",
    "        print('Google Drive has been mounted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Resize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.21.6\n"
     ]
    }
   ],
   "source": [
    "# check that numpy is using 1.21.6 or MTCNN will not work\n",
    "!pip show numpy | grep 'Version:' | cut -d ' ' -f 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ON_COLAB = 'google.colab' in str(get_ipython())\n",
    "RANDOM_SEED = 310123\n",
    "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "WORKERS = int(os.cpu_count() / 2) \n",
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "if ON_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    DARK_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/dark_undersampled' # '/content/drive/MyDrive/xai_faces/dark_undersampled_abridged_cropped'\n",
    "    LIGHT_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/light_undersampled' # '/content/drive/MyDrive/xai_faces/light_undersampled_abridged_cropped'\n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "    DARK_UNDERSAMPLED_PATH = '../data/dark_undersampled' # 'data/dark_undersampled_abridged_cropped'\n",
    "    LIGHT_UNDERSAMPLED_PATH = '../data/light_undersampled' # 'data/light_undersampled_abridged_cropped/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Batch Size: {BATCH_SIZE}')\n",
    "print(f'Workers: {WORKERS}')\n",
    "print(f'Device: {DEVICE}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cropping Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEW_SIZE = (512, 512)\n",
    "CROP_SIZE = 256\n",
    "\n",
    "# help(MTCNN)\n",
    "\n",
    "# most of these values are defaults, but I'm including them here for clarity\n",
    "mtcnn = MTCNN(\n",
    "    image_size = CROP_SIZE, \n",
    "    margin = 0, \n",
    "    in_face_size = 20, \n",
    "    thresholds = [0.6, 0.7, 0.7], \n",
    "    factor = 0.709, \n",
    "    post_process = True, \n",
    "    select_largest = True, \n",
    "    device = DEVICE) # image_size = 160"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dark Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(DARK_UNDERSAMPLED_PATH, transform = Resize(NEW_SIZE))\n",
    "\n",
    "dataset.samples = [\n",
    "    (p, p.replace(DARK_UNDERSAMPLED_PATH, DARK_UNDERSAMPLED_PATH + '_cropped'))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers = WORKERS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = training.collate_pil\n",
    ")\n",
    "\n",
    "# prevent warnings from cropping images\n",
    "with np.testing.suppress_warnings() as sup:\n",
    "    sup.filter(category = np.VisibleDeprecationWarning)\n",
    "    for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
    "        mtcnn(x, save_path = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light Undersampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ImageFolder(DARK_UNDERSAMPLED_PATH, transform = Resize(NEW_SIZE))\n",
    "\n",
    "dataset.samples = [\n",
    "    (p, p.replace(DARK_UNDERSAMPLED_PATH, DARK_UNDERSAMPLED_PATH + '_cropped'))\n",
    "        for p, _ in dataset.samples\n",
    "]\n",
    "\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    num_workers = WORKERS,\n",
    "    batch_size = BATCH_SIZE,\n",
    "    collate_fn = training.collate_pil\n",
    ")\n",
    "\n",
    "# prevent warnings from cropping images\n",
    "with np.testing.suppress_warnings() as sup:\n",
    "    sup.filter(category = np.VisibleDeprecationWarning)\n",
    "    for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
    "        mtcnn(x, save_path = y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Train/Val Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not running on Google Colab\n"
     ]
    }
   ],
   "source": [
    "if ON_COLAB:\n",
    "    print(\"Running on Google Colab\")\n",
    "    DARK_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/dark_undersampled.csv'\n",
    "    LIGHT_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/light_undersampled.csv' \n",
    "else:\n",
    "    print(\"Not running on Google Colab\")\n",
    "    DARK_UNDERSAMPLED_PATH = '../data/dark_undersampled.csv'\n",
    "    LIGHT_UNDERSAMPLED_PATH = '../data/light_undersampled.csv' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dark_undersampled_df = pd.read_csv(DARK_UNDERSAMPLED_PATH)\n",
    "\n",
    "# if ran locally, path requires '../data/' prefix\n",
    "dark_undersampled_df = dark_undersampled_df.assign(\n",
    "    image_path_full = lambda x: 'dark_undersampled/' + x['human_id'].astype(str) + '/' + x['render_id'].astype(str) + '.cam_default.f_1.rgb.png',\n",
    ")\n",
    "\n",
    "# Group the dataframe by the label\n",
    "dark_grouped_df = dark_undersampled_df.groupby('skin_labels')\n",
    "\n",
    "# Calculate the number of instances to sample from each group\n",
    "dark_group_counts = dark_grouped_df['image_path_full'].count()\n",
    "dark_sample_counts = (dark_group_counts * 0.8).astype(int)\n",
    "\n",
    "# Create a list to store the train and validation dataframes\n",
    "train_dfs, val_dfs = [], []\n",
    "\n",
    "# Loop through each group and split it into training and validation sets\n",
    "for name, group in dark_grouped_df:\n",
    "    group_sample = group.sample(min(len(group), dark_group_counts[name]), random_state = RANDOM_SEED)\n",
    "    train_group, val_group = train_test_split(group_sample, test_size = 0.2)\n",
    "    train_dfs.append(train_group)\n",
    "    val_dfs.append(val_group)\n",
    "\n",
    "# Concatenate the training and validation dataframes\n",
    "dark_undersampled_train_idx = pd.concat(train_dfs, ignore_index = True) # light: 0.598465  dark: 0.401535\n",
    "dark_undersampled_val_idx = pd.concat(val_dfs, ignore_index = True) # light: 0.598465  dark: 0.401535\n",
    "\n",
    "dark_undersampled_train_idx.to_csv('../data/dark_train_split.csv', index = False)\n",
    "dark_undersampled_val_idx.to_csv('../data/dark_val_split.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "light_undersampled_df = pd.read_csv(LIGHT_UNDERSAMPLED_PATH)\n",
    "\n",
    "# if ran locally, path requires '../data/' prefix\n",
    "light_undersampled_df = light_undersampled_df.assign(\n",
    "    image_path_full = lambda x: 'light_undersampled/' + x['human_id'].astype(str) + '/' + x['render_id'].astype(str) + '.cam_default.f_1.rgb.png',\n",
    ")\n",
    "\n",
    "# Group the dataframe by the label\n",
    "light_grouped_df = light_undersampled_df.groupby('skin_labels')\n",
    "\n",
    "# Calculate the number of instances to sample from each group\n",
    "light_group_counts = light_grouped_df['image_path_full'].count()\n",
    "light_sample_counts = (light_group_counts * 0.8).astype(int)\n",
    "\n",
    "# Create a list to store the train and validation dataframes\n",
    "train_dfs, val_dfs = [], []\n",
    "\n",
    "# Loop through each group and split it into training and validation sets\n",
    "for name, group in light_grouped_df:\n",
    "    group_sample = group.sample(min(len(group), light_group_counts[name]), random_state = RANDOM_SEED)\n",
    "    train_group, val_group = train_test_split(group_sample, test_size = 0.2)\n",
    "    train_dfs.append(train_group)\n",
    "    val_dfs.append(val_group)\n",
    "\n",
    "# Concatenate the training and validation dataframes\n",
    "light_undersampled_train_idx = pd.concat(train_dfs, ignore_index = True) # light: 0.537542  dark: 0.462458\n",
    "light_undersampled_val_idx = pd.concat(val_dfs, ignore_index = True) # light: 0.538462  dark: 0.461538\n",
    "\n",
    "light_undersampled_train_idx.to_csv('../data/light_train_split.csv', index = False)\n",
    "light_undersampled_val_idx.to_csv('../data/light_val_split.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0b3ddb784223393a36a60b90c018e23881489e0e14d1a418ff8277d4e1a929d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
