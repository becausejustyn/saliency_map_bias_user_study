{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uU6wLJtNU6pY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae33e46d-76d2-4225-d762-e9b98749d3bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q facenet_pytorch"
      ],
      "metadata": {
        "id": "aDmXBMabV1oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler, Dataset\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Resize\n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, fixed_image_standardization, training\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "QF6IhL0oVv75"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show facenet-pytorch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YABfLUXLub2f",
        "outputId": "5cc85857-8698-43ee-a057-bf6f8d5e120b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: facenet-pytorch\n",
            "Version: 2.5.2\n",
            "Summary: Pretrained Pytorch face detection and recognition models\n",
            "Home-page: https://github.com/timesler/facenet-pytorch\n",
            "Author: Tim Esler\n",
            "Author-email: tim.esler@gmail.com\n",
            "License: UNKNOWN\n",
            "Location: /usr/local/lib/python3.8/dist-packages\n",
            "Requires: numpy, pillow, requests, torchvision\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ON_COLAB = 'google.colab' in str(get_ipython())\n",
        "RANDOM_SEED = 310123\n",
        "BATCH_SIZE = 256 if torch.cuda.is_available() else 64\n",
        "\n",
        "EPOCHS = 10\n",
        "LEARNING_RATE = 1e-3\n",
        "\n",
        "WORKERS = int(os.cpu_count() / 2)\n",
        "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "if ON_COLAB:\n",
        "    print(\"Running on Google Colab\")\n",
        "    DARK_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/dark_undersampled' # '/content/drive/MyDrive/xai_faces/dark_undersampled_abridged_cropped'\n",
        "    LIGHT_UNDERSAMPLED_PATH = '/content/drive/MyDrive/xai_faces/light_undersampled' # '/content/drive/MyDrive/xai_faces/light_undersampled_abridged_cropped'\n",
        "else:\n",
        "    print(\"Not running on Google Colab\")\n",
        "    DARK_UNDERSAMPLED_PATH = '../data/dark_undersampled' # 'data/dark_undersampled_abridged_cropped'\n",
        "    LIGHT_UNDERSAMPLED_PATH = '../data/light_undersampled' # 'data/light_undersampled_abridged_cropped/'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JlllRG3gV5fQ",
        "outputId": "0c4bea53-4693-4211-b50b-45d46a4fdae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on Google Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing the Images\n",
        "\n",
        "Cropping"
      ],
      "metadata": {
        "id": "Tmcg8hPJZ1Qb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Light Undersampled"
      ],
      "metadata": {
        "id": "Y5OOhKHTaAHw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# help(MTCNN)\n",
        "\n",
        "# for preprocessing the images\n",
        "# help(MTCNN)\n",
        "mtcnn = MTCNN(image_size = 256, margin=0, min_face_size = 20, thresholds = [0.6, 0.7, 0.7], factor = 0.709, post_process = True, select_largest = True, device = DEVICE) # image_size = 160\n"
      ],
      "metadata": {
        "id": "dYgTrKLeriZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(LIGHT_UNDERSAMPLED_PATH, transform = Resize((512, 512)))\n",
        "\n",
        "dataset.samples = [\n",
        "    (p, p.replace(LIGHT_UNDERSAMPLED_PATH, LIGHT_UNDERSAMPLED_PATH + '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers = WORKERS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    collate_fn = training.collate_pil\n",
        ")"
      ],
      "metadata": {
        "id": "uvDGAgD0V5cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "OoUdhn4bsBM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
        "        mtcnn(x, save_path = y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-TZavNQr-1s",
        "outputId": "c52e1b39-e3af-4fde-ab70-6e390354711d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/30 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  boxes = np.array(boxes)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  probs = np.array(probs)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  points = np.array(points)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:444: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_boxes = np.array(selected_boxes)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:446: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_points = np.array(selected_points)\n",
            "100%|██████████| 30/30 [10:09<00:00, 20.31s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dark Undersampled"
      ],
      "metadata": {
        "id": "dp0t_jyJZ8tn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = ImageFolder(DARK_UNDERSAMPLED_PATH, transform = Resize((512, 512)))\n",
        "\n",
        "dataset.samples = [\n",
        "    (p, p.replace(DARK_UNDERSAMPLED_PATH, DARK_UNDERSAMPLED_PATH + '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers = WORKERS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    collate_fn = training.collate_pil\n",
        ")"
      ],
      "metadata": {
        "id": "YUhIygqTV5J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prevent warnings from cropping images\n",
        "with np.testing.suppress_warnings() as sup:\n",
        "    sup.filter(category = np.VisibleDeprecationWarning)\n",
        "    for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
        "        mtcnn(x, save_path = y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqCN9dSqsjCC",
        "outputId": "79cdc242-f1ce-4b17-d2b3-892cb88f2d50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 31/31 [10:13<00:00, 19.80s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Full"
      ],
      "metadata": {
        "id": "zV5L4TxpBHtJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "mD3hb92aBM97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FULL_PATH = '/content/drive/MyDrive/xai_faces/diverse_human_faces'\n",
        "\n",
        "dataset = ImageFolder(FULL_PATH, transform = Resize((512, 512)))\n",
        "\n",
        "dataset.samples = [\n",
        "    (p, p.replace(FULL_PATH, FULL_PATH + '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers = WORKERS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    collate_fn = training.collate_pil\n",
        ")"
      ],
      "metadata": {
        "id": "Gsx9ax_fBI1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
        "        mtcnn(x, save_path = y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9it1dh4BPpJ",
        "outputId": "645ecd6e-a2ae-4f34-e056-468a10e71565"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/107 [00:00<?, ?it/s]/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/utils/detect_face.py:183: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  batch_boxes, batch_points = np.array(batch_boxes), np.array(batch_points)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:339: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  boxes = np.array(boxes)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:340: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  probs = np.array(probs)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:341: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  points = np.array(points)\n",
            "  1%|          | 1/107 [00:19<34:14, 19.38s/it]/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:444: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_boxes = np.array(selected_boxes)\n",
            "/usr/local/lib/python3.8/dist-packages/facenet_pytorch/models/mtcnn.py:446: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  selected_points = np.array(selected_points)\n",
            "100%|██████████| 107/107 [55:35<00:00, 31.17s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(image_size = 256, margin=0, min_face_size = 1, thresholds = [0.1], factor = 0.709, post_process = True, select_largest = True, device = DEVICE)"
      ],
      "metadata": {
        "id": "kCRiEh1_XJJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "zsdJS3RLXJFF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/drive/MyDrive/xai_faces/"
      ],
      "metadata": {
        "id": "F66UcQ7IYw6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MISSING_PATH = '/content/drive/MyDrive/xai_faces/missing_images'\n",
        "\n",
        "dataset = ImageFolder(MISSING_PATH, transform = Resize((512, 512)))\n",
        "\n",
        "dataset.samples = [\n",
        "    (p, p.replace(MISSING_PATH, MISSING_PATH + '_cropped'))\n",
        "        for p, _ in dataset.samples\n",
        "]\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    num_workers = WORKERS,\n",
        "    batch_size = BATCH_SIZE,\n",
        "    collate_fn = training.collate_pil\n",
        ")\n",
        "\n",
        "with np.testing.suppress_warnings() as sup:\n",
        "    sup.filter(category = np.VisibleDeprecationWarning)\n",
        "    for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
        "        mtcnn(x, save_path = y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJNKzhSrXJHS",
        "outputId": "bc8ec787-f7b5-452f-d89a-c9d87a9ececd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with np.testing.suppress_warnings() as sup:\n",
        "    sup.filter(category = np.VisibleDeprecationWarning)\n",
        "    for i, (x, y) in tqdm(enumerate(loader), total = len(loader)):\n",
        "        mtcnn(x, save_path = y)"
      ],
      "metadata": {
        "id": "U3uT3MaHXJCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-HRx1TqDXJAL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}